{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dota Chat Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the data starts by loading in all the files required, which in this case are only 'chat.csv' and 'match.csv'. These contain the chat messages of each player, which team they were on and whether team 1 (radiant team) won or lost. However, these tables contain too much unnecessary information, so some cleanup is required first. This is handled by the 'merge_data' method, kept in a seperate function for seperation of code. Here, for each match of which some messages are recorded (49771 out of 50000) is stored in a row. A row consists of 4 values: match_id, radiant_win, chat_t1, chat_t2. radiant_win corresponds to whether team 1 won or lost. Prediction becomes a lot easier if both teams' chat messages are analysed, so both are included in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chat_data = pd.read_csv('data/chat.csv')\n",
    "match_data = pd.read_csv('data/match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>key</th>\n",
       "      <th>slot</th>\n",
       "      <th>time</th>\n",
       "      <th>unit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>force it</td>\n",
       "      <td>6</td>\n",
       "      <td>-8</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>wahaha</td>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>hah</td>\n",
       "      <td>1</td>\n",
       "      <td>2027</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>space boys</td>\n",
       "      <td>6</td>\n",
       "      <td>2030</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>haha</td>\n",
       "      <td>6</td>\n",
       "      <td>2103</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>hah</td>\n",
       "      <td>1</td>\n",
       "      <td>2112</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>hah</td>\n",
       "      <td>1</td>\n",
       "      <td>2133</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>we are losing</td>\n",
       "      <td>6</td>\n",
       "      <td>2141</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>hahah</td>\n",
       "      <td>1</td>\n",
       "      <td>2142</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>alche</td>\n",
       "      <td>1</td>\n",
       "      <td>2143</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>hahahhaHAHA</td>\n",
       "      <td>1</td>\n",
       "      <td>2145</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>weahaha</td>\n",
       "      <td>6</td>\n",
       "      <td>2009</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>but we dive much any other</td>\n",
       "      <td>6</td>\n",
       "      <td>2146</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>u dont have a 6k mmr player with u</td>\n",
       "      <td>6</td>\n",
       "      <td>2312</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>lol</td>\n",
       "      <td>2</td>\n",
       "      <td>2319</td>\n",
       "      <td>Trash!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>wlwlwlwllww</td>\n",
       "      <td>9</td>\n",
       "      <td>2323</td>\n",
       "      <td>u didnt see who highest here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>torso highest here</td>\n",
       "      <td>9</td>\n",
       "      <td>2335</td>\n",
       "      <td>u didnt see who highest here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>wahaha</td>\n",
       "      <td>6</td>\n",
       "      <td>2377</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>gG</td>\n",
       "      <td>1</td>\n",
       "      <td>2382</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>6k slayer</td>\n",
       "      <td>0</td>\n",
       "      <td>2387</td>\n",
       "      <td>Double T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>haha</td>\n",
       "      <td>0</td>\n",
       "      <td>2388</td>\n",
       "      <td>Double T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>no 6k</td>\n",
       "      <td>6</td>\n",
       "      <td>2391</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>no one to slay</td>\n",
       "      <td>6</td>\n",
       "      <td>2394</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>u guys are lucky</td>\n",
       "      <td>6</td>\n",
       "      <td>2307</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>tot this casual banter.</td>\n",
       "      <td>7</td>\n",
       "      <td>1931</td>\n",
       "      <td>ｔｏｍｉａ～♥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>really ?</td>\n",
       "      <td>2</td>\n",
       "      <td>2360</td>\n",
       "      <td>Trash!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>.l.</td>\n",
       "      <td>1</td>\n",
       "      <td>1919</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>space created</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hah</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ez 500</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6k Slayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439475</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2791</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439477</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2793</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439483</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2796</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439479</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez? haha</td>\n",
       "      <td>1</td>\n",
       "      <td>2793</td>\n",
       "      <td>StopDancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439480</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2794</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439481</th>\n",
       "      <td>49999</td>\n",
       "      <td>Hahahaha</td>\n",
       "      <td>7</td>\n",
       "      <td>2795</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439482</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2795</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439471</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2788</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439484</th>\n",
       "      <td>49999</td>\n",
       "      <td>ya ya so ezx</td>\n",
       "      <td>1</td>\n",
       "      <td>2796</td>\n",
       "      <td>StopDancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439485</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2797</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439478</th>\n",
       "      <td>49999</td>\n",
       "      <td>ggwp</td>\n",
       "      <td>6</td>\n",
       "      <td>2793</td>\n",
       "      <td>Dante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439470</th>\n",
       "      <td>49999</td>\n",
       "      <td>happu def noob</td>\n",
       "      <td>8</td>\n",
       "      <td>2647</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439464</th>\n",
       "      <td>49999</td>\n",
       "      <td>fuck we wait</td>\n",
       "      <td>3</td>\n",
       "      <td>2452</td>\n",
       "      <td>PINAGPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439468</th>\n",
       "      <td>49999</td>\n",
       "      <td>noob</td>\n",
       "      <td>7</td>\n",
       "      <td>2629</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439486</th>\n",
       "      <td>49999</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>1</td>\n",
       "      <td>2798</td>\n",
       "      <td>StopDancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439454</th>\n",
       "      <td>49999</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>-38</td>\n",
       "      <td>PINAGPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439455</th>\n",
       "      <td>49999</td>\n",
       "      <td>hahaha</td>\n",
       "      <td>7</td>\n",
       "      <td>385</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439456</th>\n",
       "      <td>49999</td>\n",
       "      <td>go nc</td>\n",
       "      <td>8</td>\n",
       "      <td>825</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439457</th>\n",
       "      <td>49999</td>\n",
       "      <td>furion nc</td>\n",
       "      <td>8</td>\n",
       "      <td>827</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439458</th>\n",
       "      <td>49999</td>\n",
       "      <td>medusa</td>\n",
       "      <td>3</td>\n",
       "      <td>1206</td>\n",
       "      <td>PINAGPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439469</th>\n",
       "      <td>49999</td>\n",
       "      <td>booo</td>\n",
       "      <td>7</td>\n",
       "      <td>2634</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439459</th>\n",
       "      <td>49999</td>\n",
       "      <td>noob</td>\n",
       "      <td>8</td>\n",
       "      <td>1903</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439461</th>\n",
       "      <td>49999</td>\n",
       "      <td>said dont pick sniper. still wanna play</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>StopDancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439462</th>\n",
       "      <td>49999</td>\n",
       "      <td>wait for medusa</td>\n",
       "      <td>3</td>\n",
       "      <td>2450</td>\n",
       "      <td>PINAGPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439463</th>\n",
       "      <td>49999</td>\n",
       "      <td>we end</td>\n",
       "      <td>7</td>\n",
       "      <td>2450</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439465</th>\n",
       "      <td>49999</td>\n",
       "      <td>no time</td>\n",
       "      <td>7</td>\n",
       "      <td>2452</td>\n",
       "      <td>DamN.Sakura_Jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439466</th>\n",
       "      <td>49999</td>\n",
       "      <td>G</td>\n",
       "      <td>5</td>\n",
       "      <td>2452</td>\n",
       "      <td>WorstDayOfMyLife^Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439467</th>\n",
       "      <td>49999</td>\n",
       "      <td>for your tuskar</td>\n",
       "      <td>3</td>\n",
       "      <td>2452</td>\n",
       "      <td>PINAGPALA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439460</th>\n",
       "      <td>49999</td>\n",
       "      <td>wp</td>\n",
       "      <td>1</td>\n",
       "      <td>1912</td>\n",
       "      <td>StopDancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439487</th>\n",
       "      <td>49999</td>\n",
       "      <td>ez</td>\n",
       "      <td>8</td>\n",
       "      <td>2798</td>\n",
       "      <td>DamN.Pussy_Jr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1439488 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         match_id                                      key  slot  time  \\\n",
       "0               0                                 force it     6    -8   \n",
       "28              0                                   wahaha     6  2019   \n",
       "29              0                                      hah     1  2027   \n",
       "30              0                               space boys     6  2030   \n",
       "31              0                                     haha     6  2103   \n",
       "32              0                                      hah     1  2112   \n",
       "33              0                                      hah     1  2133   \n",
       "34              0                            we are losing     6  2141   \n",
       "35              0                                    hahah     1  2142   \n",
       "36              0                                    alche     1  2143   \n",
       "37              0                              hahahhaHAHA     1  2145   \n",
       "27              0                                  weahaha     6  2009   \n",
       "38              0               but we dive much any other     6  2146   \n",
       "40              0       u dont have a 6k mmr player with u     6  2312   \n",
       "41              0                                      lol     2  2319   \n",
       "42              0                              wlwlwlwllww     9  2323   \n",
       "43              0                       torso highest here     9  2335   \n",
       "45              0                                   wahaha     6  2377   \n",
       "46              0                                       gG     1  2382   \n",
       "47              0                                6k slayer     0  2387   \n",
       "48              0                                     haha     0  2388   \n",
       "49              0                                    no 6k     6  2391   \n",
       "50              0                           no one to slay     6  2394   \n",
       "39              0                         u guys are lucky     6  2307   \n",
       "26              0                  tot this casual banter.     7  1931   \n",
       "44              0                                 really ?     2  2360   \n",
       "24              0                                      .l.     1  1919   \n",
       "1               0                            space created     1     5   \n",
       "2               0                                      hah     1     6   \n",
       "3               0                                   ez 500     6     9   \n",
       "...           ...                                      ...   ...   ...   \n",
       "1439475     49999                                       ez     8  2791   \n",
       "1439477     49999                                       ez     8  2793   \n",
       "1439483     49999                                       ez     8  2796   \n",
       "1439479     49999                                 ez? haha     1  2793   \n",
       "1439480     49999                                       ez     8  2794   \n",
       "1439481     49999                                 Hahahaha     7  2795   \n",
       "1439482     49999                                       ez     8  2795   \n",
       "1439471     49999                                       ez     8  2788   \n",
       "1439484     49999                             ya ya so ezx     1  2796   \n",
       "1439485     49999                                       ez     8  2797   \n",
       "1439478     49999                                     ggwp     6  2793   \n",
       "1439470     49999                           happu def noob     8  2647   \n",
       "1439464     49999                             fuck we wait     3  2452   \n",
       "1439468     49999                                     noob     7  2629   \n",
       "1439486     49999                                   hahaha     1  2798   \n",
       "1439454     49999                                        G     3   -38   \n",
       "1439455     49999                                   hahaha     7   385   \n",
       "1439456     49999                                    go nc     8   825   \n",
       "1439457     49999                                furion nc     8   827   \n",
       "1439458     49999                                   medusa     3  1206   \n",
       "1439469     49999                                     booo     7  2634   \n",
       "1439459     49999                                     noob     8  1903   \n",
       "1439461     49999  said dont pick sniper. still wanna play     1  2014   \n",
       "1439462     49999                          wait for medusa     3  2450   \n",
       "1439463     49999                                   we end     7  2450   \n",
       "1439465     49999                                  no time     7  2452   \n",
       "1439466     49999                                        G     5  2452   \n",
       "1439467     49999                          for your tuskar     3  2452   \n",
       "1439460     49999                                       wp     1  1912   \n",
       "1439487     49999                                       ez     8  2798   \n",
       "\n",
       "                                  unit  \n",
       "0                            6k Slayer  \n",
       "28                           6k Slayer  \n",
       "29                              Monkey  \n",
       "30                           6k Slayer  \n",
       "31                           6k Slayer  \n",
       "32                              Monkey  \n",
       "33                              Monkey  \n",
       "34                           6k Slayer  \n",
       "35                              Monkey  \n",
       "36                              Monkey  \n",
       "37                              Monkey  \n",
       "27                           6k Slayer  \n",
       "38                           6k Slayer  \n",
       "40                           6k Slayer  \n",
       "41                            Trash!!!  \n",
       "42       u didnt see who highest here?  \n",
       "43       u didnt see who highest here?  \n",
       "45                           6k Slayer  \n",
       "46                              Monkey  \n",
       "47                            Double T  \n",
       "48                            Double T  \n",
       "49                           6k Slayer  \n",
       "50                           6k Slayer  \n",
       "39                           6k Slayer  \n",
       "26                             ｔｏｍｉａ～♥  \n",
       "44                            Trash!!!  \n",
       "24                              Monkey  \n",
       "1                               Monkey  \n",
       "2                               Monkey  \n",
       "3                            6k Slayer  \n",
       "...                                ...  \n",
       "1439475                  DamN.Pussy_Jr  \n",
       "1439477                  DamN.Pussy_Jr  \n",
       "1439483                  DamN.Pussy_Jr  \n",
       "1439479                    StopDancing  \n",
       "1439480                  DamN.Pussy_Jr  \n",
       "1439481                 DamN.Sakura_Jr  \n",
       "1439482                  DamN.Pussy_Jr  \n",
       "1439471                  DamN.Pussy_Jr  \n",
       "1439484                    StopDancing  \n",
       "1439485                  DamN.Pussy_Jr  \n",
       "1439478                          Dante  \n",
       "1439470                  DamN.Pussy_Jr  \n",
       "1439464                      PINAGPALA  \n",
       "1439468                 DamN.Sakura_Jr  \n",
       "1439486                    StopDancing  \n",
       "1439454                      PINAGPALA  \n",
       "1439455                 DamN.Sakura_Jr  \n",
       "1439456                  DamN.Pussy_Jr  \n",
       "1439457                  DamN.Pussy_Jr  \n",
       "1439458                      PINAGPALA  \n",
       "1439469                 DamN.Sakura_Jr  \n",
       "1439459                  DamN.Pussy_Jr  \n",
       "1439461                    StopDancing  \n",
       "1439462                      PINAGPALA  \n",
       "1439463                 DamN.Sakura_Jr  \n",
       "1439465                 DamN.Sakura_Jr  \n",
       "1439466          WorstDayOfMyLife^Ever  \n",
       "1439467                      PINAGPALA  \n",
       "1439460                    StopDancing  \n",
       "1439487                  DamN.Pussy_Jr  \n",
       "\n",
       "[1439488 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data.sort_values(by=['match_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(chat, match):\n",
    "    total_chat_t1 = []\n",
    "    total_chat_t2 = []\n",
    "    chat_t1 = []\n",
    "    chat_t2 = []\n",
    "    matches = []\n",
    "    wins = []\n",
    "    curr_match = chat_data['match_id'][0]\n",
    "    for index, row in chat_data.iterrows():\n",
    "        if curr_match != row['match_id']:\n",
    "            wins.append(match['radiant_win'][curr_match])\n",
    "            matches.append(curr_match)\n",
    "            total_chat_t1.append(chat_t1)\n",
    "            total_chat_t2.append(chat_t2)\n",
    "            chat_t1 = []\n",
    "            chat_t2 = []\n",
    "        if row['slot'] <= 4:\n",
    "            chat_t1.append(row['key'])\n",
    "        else:\n",
    "            chat_t2.append(row['key'])\n",
    "        curr_match = row['match_id']\n",
    "    wins.append(match['radiant_win'][curr_match])\n",
    "    matches.append(curr_match)\n",
    "    total_chat_t1.append(chat_t1)\n",
    "    total_chat_t2.append(chat_t2)\n",
    "    print(len(matches[1:]), len(wins[1:]), len(total_chat_t1[1:]), len(total_chat_t2[1:]))\n",
    "    data = {'match_id': matches, 'radiant_win': wins, 'chat_t1': total_chat_t1, 'chat_t2': total_chat_t2}\n",
    "    data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49771 49771 49771 49771\n"
     ]
    }
   ],
   "source": [
    "df = merge_data(chat_data, match_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>radiant_win</th>\n",
       "      <th>chat_t1</th>\n",
       "      <th>chat_t2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[space created, hah, mvp ulti, hah, fuck my as...</td>\n",
       "      <td>[force it, ez 500, bye, fate, is cruel, sad sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[lol, gege, our eshaker afk, 4 v 4, it will be...</td>\n",
       "      <td>[why, he afk, ya, gg, commend the solo supp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>[no, not my problem...i guess, XD, we will wai...</td>\n",
       "      <td>[w8 1 min plz, 1 min, ok dude, so kind of you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>[we dno, hi, ty, u2, bait, no luck, just skill...</td>\n",
       "      <td>[how long, dude, we wait him 10 mints, .., Wtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>[gamegood, gg, gfg]</td>\n",
       "      <td>[gg, gg wr, sad wr sad life, ez mid for qop]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_id  radiant_win                                            chat_t1  \\\n",
       "0         0         True  [space created, hah, mvp ulti, hah, fuck my as...   \n",
       "1         1        False  [lol, gege, our eshaker afk, 4 v 4, it will be...   \n",
       "2         2        False  [no, not my problem...i guess, XD, we will wai...   \n",
       "3         3        False  [we dno, hi, ty, u2, bait, no luck, just skill...   \n",
       "4         4         True                                [gamegood, gg, gfg]   \n",
       "\n",
       "                                             chat_t2  \n",
       "0  [force it, ez 500, bye, fate, is cruel, sad sp...  \n",
       "1       [why, he afk, ya, gg, commend the solo supp]  \n",
       "2  [w8 1 min plz, 1 min, ok dude, so kind of you,...  \n",
       "3  [how long, dude, we wait him 10 mints, .., Wtf...  \n",
       "4       [gg, gg wr, sad wr sad life, ez mid for qop]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transforming the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice DataFrame with our required data, we can start turning into a set of feature vectors and word vectors. This is because our model will not be able to handle raw strings without conversion. For this purpose, we first scrap an online web page (netlingo.com) with a lot of different abbreviations. This code was not created by us (except for changing case to lower and *** to uck, but it is quite general and applies here quite nicely. The data is then stored in a json file labeled 'slang.json' for later use. Next, we also import emot, a library that can detect emojis and emoticons (useful in our case) in text. From these two sets of dictionaries, we now iterate through the data we generated and start replacing values. This will make our text more generic and close to english. It will also remove a lot of context, which is not important in short messages (most are no longer than 2 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_netlingo():\n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests, json\n",
    "    resp = requests.get('http://www.netlingo.com/acronyms.php')\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    slangdict= {}\n",
    "    key=\"\"\n",
    "    value=\"\"\n",
    "    for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "        for li in div.findAll('li'):\n",
    "            for a in li.findAll('a'):\n",
    "                key = a.text\n",
    "                value = li.text.split(key)[1]\n",
    "                # fuck is always changed to f***, so uck is used instead\n",
    "                slangdict[key.lower()]= re.sub('\\*\\*\\*', 'uck', value.lower())\n",
    "    with open('data/slang.json', 'w') as fid:\n",
    "        json.dump(slangdict,fid,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_netlingo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/slang.json') as json_file:  \n",
    "    slang = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_slang(sentence):\n",
    "    import re\n",
    "    sentence = str(sentence).lower()\n",
    "    #replace laughter with lol\n",
    "    sentence = re.sub(r'\\b(a*ha+h[ha]*|o?l+o+l+[ol]*)\\b', 'lol', sentence)\n",
    "    #replace question marks with just one\n",
    "    sentence = re.sub(r'\\?+', ' ?', sentence)\n",
    "    s = sentence.split()\n",
    "    new_s = []\n",
    "    for word in s:\n",
    "        if word in slang:\n",
    "            if \"it means\" in slang[word] and \"-or-\" in slang[word]:\n",
    "                new_s.append(slang[word][8:slang[word].index(\"-or-\") - 1])\n",
    "                continue\n",
    "            if \"it means\" in slang[word]:\n",
    "                new_s.append(slang[word][8:])\n",
    "                continue\n",
    "            if \"-or-\" in slang[word]:\n",
    "                new_s.append(slang[word][:slang[word].index(\"-or-\") - 1])\n",
    "                continue\n",
    "            new_s.append(slang[word])\n",
    "        else:\n",
    "            new_s.append(word)\n",
    "    return new_s\n",
    "\n",
    "def replace_emoticons(sentence):\n",
    "    import emot\n",
    "    #assumes the sentence is split already\n",
    "    new_s = []\n",
    "    for word in sentence:\n",
    "        if type(emot.emoticons(word)) == dict and emot.emoticons(word)['mean'] != []:\n",
    "            new_s.append(''.join(emot.emoticons(word)['mean']))\n",
    "        else:\n",
    "            new_s.append(word)\n",
    "    return new_s\n",
    "\n",
    "def replace_special_characters(sentence):\n",
    "    sentence = replace_slang(sentence)\n",
    "    sentence = replace_emoticons(sentence)\n",
    "    return sentence\n",
    "\n",
    "def replace_special_characters_in_list(l):\n",
    "    new_l = []\n",
    "    for sentence in l:\n",
    "        new_l.append(replace_special_characters(sentence))\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_copy['chat_t1'][0])\n",
    "# print(replace_special_characters_in_list(df_copy['chat_t1'][0]))\n",
    "df_copy['chat_t1'] = df_copy['chat_t1'].apply(replace_special_characters_in_list)\n",
    "df_copy['chat_t2'] = df_copy['chat_t2'].apply(replace_special_characters_in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Use the same random state to ensure no difference in rows selected\n",
    "train_vectors_t1, test_vectors_t1, train_labels_t1, test_labels_t1 = train_test_split(df_copy['chat_t1'], df_copy['radiant_win'], random_state = 123, test_size=0.1)\n",
    "train_vectors_t2, test_vectors_t2, train_labels_t2, test_labels_t2 = train_test_split(df_copy['chat_t1'], df_copy['radiant_win'], random_state = 123, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_words = []\n",
    "for index, row in enumerate(train_vectors_t1):\n",
    "    for sentence in train_vectors_t1.iloc[index]:\n",
    "        training_words.append(sentence)\n",
    "    for sentence in train_vectors_t2.iloc[index]:\n",
    "        training_words.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.18272698, -0.3214452 , -0.43639842,  0.06477252, -0.05136469,\n",
       "       -0.39871436, -0.9818467 , -1.5398655 ,  0.2590159 , -0.6979984 ,\n",
       "       -1.3583335 ,  0.00358243, -0.34315267,  0.24765311,  0.7159464 ,\n",
       "        0.5078792 , -1.0051621 , -0.54715174,  0.35380465, -0.53530425,\n",
       "       -1.6304233 ,  0.7083461 ,  0.5425161 ,  1.4555992 , -0.8925635 ,\n",
       "       -0.13102978, -0.19437468, -0.9377924 , -0.609619  , -0.06792807,\n",
       "        0.4237146 ,  0.43225282, -0.01834104,  0.2349232 , -0.39721063,\n",
       "        0.5616004 , -0.41507465,  0.7218995 ,  0.9010574 , -0.8152847 ,\n",
       "       -0.12789036, -0.74315065,  0.01354121, -0.42839342,  0.07912259,\n",
       "        0.8706909 , -0.55707514, -0.1153044 , -0.12271321, -0.78538334,\n",
       "        0.06370596, -0.35504368, -0.12932321,  0.55758536, -0.1491564 ,\n",
       "        0.79510057,  0.10631128, -0.30056137, -0.50617033,  1.227375  ,\n",
       "        0.53664625,  0.274932  , -0.35916793,  0.87219876,  0.8842256 ,\n",
       "       -0.40891075,  1.3360981 ,  0.02331601, -0.565052  , -0.02653057,\n",
       "       -0.17407805, -1.8022826 , -0.83406276,  0.08938238,  1.4853545 ,\n",
       "        0.40032804, -2.2809265 ,  0.09958135, -0.18862773,  0.6941662 ,\n",
       "        0.8242494 ,  1.2999327 , -0.35499945,  0.21097173,  0.98103964,\n",
       "        1.3586199 , -0.1484555 ,  1.0231484 , -1.0555784 ,  0.9458081 ,\n",
       "        0.17752923, -1.0994289 , -0.37085286, -0.8007133 ,  0.03829212,\n",
       "       -0.18752289, -1.037525  , -0.2790981 , -0.38320562,  0.31704816],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(training_words, window=3, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('decent', 0.830140233039856),\n",
       " ('good', 0.8273137807846069),\n",
       " ('amazing', 0.7752606868743896),\n",
       " ('sick', 0.7691141366958618),\n",
       " ('awesome', 0.762021541595459),\n",
       " ('perfect', 0.7480865120887756),\n",
       " ('terrible', 0.746741771697998),\n",
       " ('cool', 0.7368814945220947),\n",
       " ('big', 0.7329425811767578),\n",
       " ('shitty', 0.7326442003250122)]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wow, those are some ridiculously good results. Try some yourself, like 'laughing out loud' or 'amazing'! (also notice 'shitty', maybe because of sarcasm?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try to make use of SVM. Thanks to my previous experiences with it. it has gained my respect as a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentence):\n",
    "    global model\n",
    "    s = []\n",
    "    n_unknowns = 0\n",
    "    for w in sentence:\n",
    "        if w == \"<UNKNOWN>\":\n",
    "#             s.append(np.random.rand(model.vector_size))\n",
    "            n_unknowns+= 1\n",
    "        else:\n",
    "            s.append(model[w])\n",
    "    vector = []\n",
    "    if n_unknowns == len(sentence):\n",
    "        return [0] * model.vector_size\n",
    "    for i in range(0, len(s[0])):\n",
    "        count = 0\n",
    "        for word in s:\n",
    "            count+= word[i]\n",
    "        vector.append(count/(len(s)))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(v):\n",
    "    all_vectors = []\n",
    "    for i in range(0,len(v[0])):\n",
    "        vector = []\n",
    "        sum = 0\n",
    "        for j in range(0, len(v)):\n",
    "            sum+= v[j][i]\n",
    "        vector.append(sum / len(v))\n",
    "        all_vectors.append(vector)\n",
    "    return all_vectors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "chat_t1_vectors = []\n",
    "chat_t2_vectors = []\n",
    "for index, row in train_vectors_t1.iteritems():\n",
    "    chat_t1_vector = []\n",
    "    chat_t2_vector = []\n",
    "    if len(train_vectors_t1[index]) == 0:\n",
    "        chat_t1_vectors.append([0] * 100)\n",
    "    else:\n",
    "        for sentence in train_vectors_t1[index]:\n",
    "            chat_t1_vector.append(sent2vec(sentence))\n",
    "        chat_t1_vectors.append(to_vector(chat_t1_vector))\n",
    "    if len(train_vectors_t2[index]) == 0:\n",
    "        chat_t2_vectors.append([0] * 100)\n",
    "    else:\n",
    "        for sentence in train_vectors_t2[index]:\n",
    "            chat_t2_vector.append(sent2vec(sentence))\n",
    "        chat_t2_vectors.append(to_vector(chat_t2_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector2(v1, v2):\n",
    "    all_vectors = []\n",
    "    for i in range(0,len(v1)):\n",
    "        vector = []\n",
    "        for j in range(0,len(v1[i])):\n",
    "            num1 = 0\n",
    "            if type(v1[i][j]) == list:\n",
    "                num1 = v1[i][j][0]\n",
    "            else:\n",
    "                num1 = v1[i][j]\n",
    "            num2 = 0\n",
    "            if type(v2[i][j]) == list:\n",
    "                num2 = v2[i][j][0]\n",
    "            else:\n",
    "                num2 = v2[i][j]\n",
    "            vector.append((num1 + num2) / 2)\n",
    "        all_vectors.append(vector)\n",
    "    return all_vectors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_vectors = to_vector2(chat_t1_vectors, chat_t2_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(chat_vectors, [1 if win == True else 0 for win in train_labels_t1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknowns(string, model):\n",
    "    known_words = model.wv.vocab.keys()\n",
    "    new_string = [];\n",
    "    for w in string:\n",
    "        if w not in known_words:\n",
    "            new_string.append(\"<UNKNOWN>\")\n",
    "        else:\n",
    "            new_string.append(w)\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "chat_t1_vectors_test = []\n",
    "chat_t2_vectors_test = []\n",
    "for index, row in test_vectors_t1.iteritems():\n",
    "    chat_t1_vector_test = []\n",
    "    chat_t2_vector_test = []\n",
    "    if len(test_vectors_t1[index]) == 0:\n",
    "        chat_t1_vectors_test.append([0] * 100)\n",
    "    else:\n",
    "        for sentence in test_vectors_t1[index]:\n",
    "            chat_t1_vector_test.append(sent2vec(replace_unknowns(sentence, model)))\n",
    "        chat_t1_vectors_test.append(to_vector(chat_t1_vector_test))\n",
    "    if len(test_vectors_t2[index]) == 0:\n",
    "        chat_t2_vectors_test.append([0] * 100)\n",
    "    else:\n",
    "        for sentence in test_vectors_t2[index]:\n",
    "            chat_t2_vector_test.append(sent2vec(replace_unknowns(sentence, model)))\n",
    "        chat_t2_vectors_test.append(to_vector(chat_t2_vector_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_vectors_test = to_vector2(chat_t1_vectors_test, chat_t2_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6729610285255122"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(chat_vectors_test, [1 if win == True else 0 for win in test_labels_t1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let's try logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                           multi_class='multinomial').fit(chat_vectors, [1 if win == True else 0 for win in train_labels_t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clf2.predict(chat_vectors_test)\n",
    "correct = [p == l for p, l in zip(result, test_labels_t1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6504620329449579"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct.count(True)/len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of 0.65, a bit below SVM's 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also maybe XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "train = xgb.DMatrix(chat_vectors, label=train_labels_t1)\n",
    "test = xgb.DMatrix(chat_vectors_test, label=test_labels_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 2, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "evallist = [(test, 'eval'), (train, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.636788\ttrain-auc:0.625304\n",
      "[1]\teval-auc:0.663269\ttrain-auc:0.655561\n",
      "[2]\teval-auc:0.675408\ttrain-auc:0.667183\n",
      "[3]\teval-auc:0.68004\ttrain-auc:0.672882\n",
      "[4]\teval-auc:0.684209\ttrain-auc:0.678041\n",
      "[5]\teval-auc:0.687914\ttrain-auc:0.682093\n",
      "[6]\teval-auc:0.688238\ttrain-auc:0.685862\n",
      "[7]\teval-auc:0.689838\ttrain-auc:0.689198\n",
      "[8]\teval-auc:0.690777\ttrain-auc:0.691898\n",
      "[9]\teval-auc:0.69137\ttrain-auc:0.693153\n",
      "[10]\teval-auc:0.693095\ttrain-auc:0.695434\n",
      "[11]\teval-auc:0.692787\ttrain-auc:0.697758\n",
      "[12]\teval-auc:0.691487\ttrain-auc:0.699891\n",
      "[13]\teval-auc:0.692141\ttrain-auc:0.701603\n",
      "[14]\teval-auc:0.694489\ttrain-auc:0.703251\n",
      "[15]\teval-auc:0.693896\ttrain-auc:0.704224\n",
      "[16]\teval-auc:0.69447\ttrain-auc:0.705511\n",
      "[17]\teval-auc:0.695058\ttrain-auc:0.707486\n",
      "[18]\teval-auc:0.695404\ttrain-auc:0.708936\n",
      "[19]\teval-auc:0.693989\ttrain-auc:0.710566\n",
      "[20]\teval-auc:0.693473\ttrain-auc:0.71226\n",
      "[21]\teval-auc:0.693285\ttrain-auc:0.713064\n",
      "[22]\teval-auc:0.69335\ttrain-auc:0.714173\n",
      "[23]\teval-auc:0.696076\ttrain-auc:0.715231\n",
      "[24]\teval-auc:0.695151\ttrain-auc:0.716359\n",
      "[25]\teval-auc:0.695137\ttrain-auc:0.71704\n",
      "[26]\teval-auc:0.694849\ttrain-auc:0.717809\n",
      "[27]\teval-auc:0.694423\ttrain-auc:0.718795\n",
      "[28]\teval-auc:0.693872\ttrain-auc:0.719315\n",
      "[29]\teval-auc:0.69456\ttrain-auc:0.720124\n",
      "[30]\teval-auc:0.695016\ttrain-auc:0.721165\n",
      "[31]\teval-auc:0.695844\ttrain-auc:0.721846\n",
      "[32]\teval-auc:0.69575\ttrain-auc:0.7226\n",
      "[33]\teval-auc:0.696507\ttrain-auc:0.723505\n",
      "[34]\teval-auc:0.695869\ttrain-auc:0.724371\n",
      "[35]\teval-auc:0.696553\ttrain-auc:0.725247\n",
      "[36]\teval-auc:0.697506\ttrain-auc:0.725958\n",
      "[37]\teval-auc:0.697352\ttrain-auc:0.727147\n",
      "[38]\teval-auc:0.696025\ttrain-auc:0.72757\n",
      "[39]\teval-auc:0.695501\ttrain-auc:0.72828\n",
      "[40]\teval-auc:0.698403\ttrain-auc:0.729163\n",
      "[41]\teval-auc:0.699904\ttrain-auc:0.730049\n",
      "[42]\teval-auc:0.699671\ttrain-auc:0.730418\n",
      "[43]\teval-auc:0.699764\ttrain-auc:0.730901\n",
      "[44]\teval-auc:0.700215\ttrain-auc:0.731694\n",
      "[45]\teval-auc:0.699863\ttrain-auc:0.732259\n",
      "[46]\teval-auc:0.700221\ttrain-auc:0.732844\n",
      "[47]\teval-auc:0.700423\ttrain-auc:0.733447\n",
      "[48]\teval-auc:0.701291\ttrain-auc:0.733999\n",
      "[49]\teval-auc:0.700405\ttrain-auc:0.734512\n",
      "[50]\teval-auc:0.700549\ttrain-auc:0.734912\n",
      "[51]\teval-auc:0.700865\ttrain-auc:0.735685\n",
      "[52]\teval-auc:0.700312\ttrain-auc:0.736121\n",
      "[53]\teval-auc:0.700811\ttrain-auc:0.73649\n",
      "[54]\teval-auc:0.701014\ttrain-auc:0.737095\n",
      "[55]\teval-auc:0.700193\ttrain-auc:0.737897\n",
      "[56]\teval-auc:0.700368\ttrain-auc:0.738538\n",
      "[57]\teval-auc:0.701542\ttrain-auc:0.739044\n",
      "[58]\teval-auc:0.702209\ttrain-auc:0.739509\n",
      "[59]\teval-auc:0.702012\ttrain-auc:0.740078\n",
      "[60]\teval-auc:0.701697\ttrain-auc:0.74052\n",
      "[61]\teval-auc:0.700824\ttrain-auc:0.740923\n",
      "[62]\teval-auc:0.700916\ttrain-auc:0.741468\n",
      "[63]\teval-auc:0.700099\ttrain-auc:0.74208\n",
      "[64]\teval-auc:0.700115\ttrain-auc:0.742572\n",
      "[65]\teval-auc:0.699446\ttrain-auc:0.742949\n",
      "[66]\teval-auc:0.699407\ttrain-auc:0.743439\n",
      "[67]\teval-auc:0.699661\ttrain-auc:0.74396\n",
      "[68]\teval-auc:0.700323\ttrain-auc:0.744598\n",
      "[69]\teval-auc:0.70116\ttrain-auc:0.745159\n",
      "[70]\teval-auc:0.700578\ttrain-auc:0.745694\n",
      "[71]\teval-auc:0.700709\ttrain-auc:0.746218\n",
      "[72]\teval-auc:0.70146\ttrain-auc:0.74671\n",
      "[73]\teval-auc:0.701895\ttrain-auc:0.747051\n",
      "[74]\teval-auc:0.702852\ttrain-auc:0.747613\n",
      "[75]\teval-auc:0.702444\ttrain-auc:0.748228\n",
      "[76]\teval-auc:0.702086\ttrain-auc:0.748674\n",
      "[77]\teval-auc:0.702276\ttrain-auc:0.749058\n",
      "[78]\teval-auc:0.702893\ttrain-auc:0.749448\n",
      "[79]\teval-auc:0.702809\ttrain-auc:0.74966\n",
      "[80]\teval-auc:0.704026\ttrain-auc:0.750136\n",
      "[81]\teval-auc:0.703976\ttrain-auc:0.750467\n",
      "[82]\teval-auc:0.704012\ttrain-auc:0.750782\n",
      "[83]\teval-auc:0.70396\ttrain-auc:0.751119\n",
      "[84]\teval-auc:0.702637\ttrain-auc:0.751387\n",
      "[85]\teval-auc:0.704121\ttrain-auc:0.751816\n",
      "[86]\teval-auc:0.703638\ttrain-auc:0.752007\n",
      "[87]\teval-auc:0.703675\ttrain-auc:0.752375\n",
      "[88]\teval-auc:0.703809\ttrain-auc:0.752682\n",
      "[89]\teval-auc:0.704094\ttrain-auc:0.753044\n",
      "[90]\teval-auc:0.704045\ttrain-auc:0.753303\n",
      "[91]\teval-auc:0.704158\ttrain-auc:0.753518\n",
      "[92]\teval-auc:0.70394\ttrain-auc:0.753693\n",
      "[93]\teval-auc:0.704164\ttrain-auc:0.753862\n",
      "[94]\teval-auc:0.704447\ttrain-auc:0.754236\n",
      "[95]\teval-auc:0.703915\ttrain-auc:0.754543\n",
      "[96]\teval-auc:0.703035\ttrain-auc:0.755295\n",
      "[97]\teval-auc:0.704111\ttrain-auc:0.755591\n",
      "[98]\teval-auc:0.703809\ttrain-auc:0.755932\n",
      "[99]\teval-auc:0.703329\ttrain-auc:0.756399\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "bst = xgb.train(param, train, num_round, evallist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training seems to be going quite slowly, but AUC is gradually increasing for the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
